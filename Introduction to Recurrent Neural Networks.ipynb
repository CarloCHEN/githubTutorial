{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LrbYZJDNBKYv"
   },
   "source": [
    "<div id=\"container\" style=\"position:relative;\">\n",
    "<div style=\"float:left\"><h1> Introduction to Recurrent Neural Networks </h1></div>\n",
    "<div style=\"position:relative; float:right\"><img style=\"height:65px\" src =\"https://drive.google.com/uc?export=view&id=1EnB0x-fdqMp6I5iMoEBBEuxB_s7AmE2k\" />\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2z8qJcVWBKYw"
   },
   "source": [
    "So far we have seen standard Neural Networks (NNs/ANNs) as well as Convolutional Neural Networks (CNNs). These were powerful methods, but neither have the notion of a series where each value can affect the next. What if we have a set of inputs where a series of inputs could decide the next value? For example, words in a sentence or essay, or price movements. This is where Recurrent Neural Networks (RNNs) come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2qHFwKtBKYx"
   },
   "source": [
    "Required libraries:\n",
    "1. `tensorflow`\n",
    "2. `matplotlib`\n",
    "3. `pydot`\n",
    "4. `graphviz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "6BQ8ha5VBKYy",
    "outputId": "d4a7facb-73b2-4270-ae49-fde70b38b119"
   },
   "outputs": [],
   "source": [
    "# Let's load up some libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os.path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# We will use tensorflow.tensorflow.keras in this notebook\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, BatchNormalization, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2XsQUz-HBKY2"
   },
   "source": [
    "### Series Data\n",
    "\n",
    "In series data $\\vec{X} = [x_1, x_2, ..., x_n]$ we assume that the data points are not independent. In essence, every $x_t$ has an effect on the next $x_{t+1}$ and knowing the previous data point allows us to get some information about the next data point. More loosely speaking, not only do the values carry information, but their order does as well.\n",
    "\n",
    "One of the best examples is language, where the order of the words has a tremendous effect on the change in meaning. Consider the following two sentences:\n",
    "1. A person made a neural network.\n",
    "2. A neural network made a person.\n",
    "\n",
    "The first sentence describes an ordinary occurrence in our classroom, the second sentence marks the beginning of SkyNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0bQkzlS2BKY4"
   },
   "source": [
    "#### Effect on Neural Network design\n",
    "\n",
    "From a functional perspective, we would like our neurons to be slightly modified. Whereas before we were interested in each neuron simply intaking a value, doing some processing on it, and then producing an output (right image), now we would like each neuron to take in a series. Furthermore, we'd like the output from each element in the series to be an input for the next element, and once the entire series has been inputted, we'd like to see the neuron fire (left image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EA22_utABKY5"
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=15kwpMDsF6yzZpTtaqXlZmO_c9ZOaU7tr\" width=500 height=400>\n",
    "\n",
    "<center> <i>Image source: https://www.sciencedirect.com/science/article/pii/S088523081400093X#fig0010)</i> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClHxOiDHBKY5"
   },
   "source": [
    "The basis for Recurrent Neural Networks (RNNs) are these Recurrent Neurons. We can think of each neuron as a series of neurons which take in a sequence ($x_0, x_1, ..., x_t$ in the image below), transmitting a state along the sequence of neurons (the arrows going across the series of neurons $A$), and outputting either a final output ($h_t$), or a series of outputs for another recurrent neuron to consume ($h_0, h_1, ..., h_t$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "duM-oI2RBKY6"
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=13jVNRNYmvirzCKZYld5KlbgXKtt4uJxI\" height=400 width=600>\n",
    "\n",
    "<center> <i>Image source: https://machinelearning-blog.com/2018/02/21/recurrent-neural-networks/ ) </i> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aefRQwSUeKeJ"
   },
   "source": [
    "### Effect on Data\n",
    "\n",
    "This modifies the way we feed the data into the network as well. For a regular Feed-forward NN, each observation would be composed of $d$ dimensions, and we would have a total of $n$ observations to create a dataset of $n \\times d$. However, for recurrent networks, for each dimension/feature, we would have a sequence of $q$ observations, so that each data point is a $q \\times d$ matrix. This means our entire dataset will be of shape $n \\times q \\times d$ (this 3D \"matrix\" is called a **Tensor**).\n",
    "\n",
    "So for example, if we have a dataset where every data point is made up of 4 input features, each of which is a series of 20 measurements (e.g. 20 days), then every data point is described by a $20 \\times 4$ matrix. And if we have 1,000 data points, our dataset will be of size $1000 \\times 20 \\times 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I89IUb7geKeL"
   },
   "source": [
    "## Let's write some poetry! (or at least try)\n",
    "\n",
    "Let's say we wanted to produce an RNN which can write poetry for us. We will give it some starting phrase, and we expect it to complete a poem for us. \n",
    "There are two ways to explore this problem. We can look at poems (and language) as a sequence of characters, or as a sequence of words.\n",
    "\n",
    "We will start with the simple case of character sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data into Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNk1mvGOeKeL"
   },
   "source": [
    "#### Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GNwoPKqneKeM"
   },
   "source": [
    "The most important part of every data science project is getting the data and transforming it to our required format. Let's grab a dataset of poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "LxRuHTYPeKeM",
    "outputId": "b3e4bfc4-5d2f-4312-b322-31ad3176d8b1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poetry Foundation ID</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wendy Videlock</td>\n",
       "      <td>!</td>\n",
       "      <td>55489</td>\n",
       "      <td>Dear Writers, I’m compiling the first in what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hailey Leithauser</td>\n",
       "      <td>0</td>\n",
       "      <td>41729</td>\n",
       "      <td>Philosophic\\nin its complex, ovoid emptiness,\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jody Gladding</td>\n",
       "      <td>1-800-FEAR</td>\n",
       "      <td>57135</td>\n",
       "      <td>We'd  like  to  talk  with  you  about  fear t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joseph Brodsky</td>\n",
       "      <td>1 January 1965</td>\n",
       "      <td>56736</td>\n",
       "      <td>The Wise Men will unlearn your name.\\nAbove yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ted Berrigan</td>\n",
       "      <td>3 Pages</td>\n",
       "      <td>51624</td>\n",
       "      <td>For Jack Collom\\n10 Things I do Every Day\\n\\np...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author           Title  Poetry Foundation ID  \\\n",
       "0     Wendy Videlock               !                 55489   \n",
       "1  Hailey Leithauser               0                 41729   \n",
       "2      Jody Gladding      1-800-FEAR                 57135   \n",
       "3     Joseph Brodsky  1 January 1965                 56736   \n",
       "4       Ted Berrigan         3 Pages                 51624   \n",
       "\n",
       "                                             Content  \n",
       "0  Dear Writers, I’m compiling the first in what ...  \n",
       "1  Philosophic\\nin its complex, ovoid emptiness,\\...  \n",
       "2  We'd  like  to  talk  with  you  about  fear t...  \n",
       "3  The Wise Men will unlearn your name.\\nAbove yo...  \n",
       "4  For Jack Collom\\n10 Things I do Every Day\\n\\np...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data was taken from https://www.kaggle.com/johnhallman/complete-poetryfoundationorg-dataset\n",
    "poems_df = pd.read_csv('/Users/yuanyaning/Downloads/kaggle_poem_dataset.csv', index_col=0)\n",
    "poems_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrqggWZGeKeQ"
   },
   "source": [
    "Poetry is quite interesting because it is unique to every author, so we should see if we can find an author in this dataset with a large collection of poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "AyWxiu7TeKeQ",
    "outputId": "04cf89a6-39d0-4834-a693-20f8a0db0a03",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "William Shakespeare      85\n",
       "Anonymous                82\n",
       "Alfred, Lord Tennyson    78\n",
       "Rae Armantrout           62\n",
       "William Wordsworth       59\n",
       "Name: Author, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poems_df['Author'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfyOKcf7eKeS"
   },
   "source": [
    "William Shakespeare... not terribly surprising. But in our case let's try a close second, Alfred Tennyson (the Shakespeare poems have a few strange characters in them that make things harder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "dvs4-SZYeKeT",
    "outputId": "76a4f0f0-195f-4dbf-c334-688a257208e3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poetry Foundation ID</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>Alfred, Lord Tennyson</td>\n",
       "      <td>Break, Break, Break</td>\n",
       "      <td>45318</td>\n",
       "      <td>Break, break, break,\\nOn thy cold gray stones,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>Alfred, Lord Tennyson</td>\n",
       "      <td>The Charge of the Light Brigade</td>\n",
       "      <td>45319</td>\n",
       "      <td>I\\n\\nHalf a league, half a league,\\nHalf a lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2315</th>\n",
       "      <td>Alfred, Lord Tennyson</td>\n",
       "      <td>Claribel</td>\n",
       "      <td>45320</td>\n",
       "      <td>Where Claribel low-lieth\\nThe breezes pause an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>Alfred, Lord Tennyson</td>\n",
       "      <td>Crossing the Bar\\n \\n \\n  \\n   Launch Audio in...</td>\n",
       "      <td>45321</td>\n",
       "      <td>Sunset and evening star,\\nAnd one clear call f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>Alfred, Lord Tennyson</td>\n",
       "      <td>The Eagle</td>\n",
       "      <td>45322</td>\n",
       "      <td>He clasps the crag with crooked hands;\\nClose ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Author  \\\n",
       "1730  Alfred, Lord Tennyson   \n",
       "2134  Alfred, Lord Tennyson   \n",
       "2315  Alfred, Lord Tennyson   \n",
       "2687  Alfred, Lord Tennyson   \n",
       "3529  Alfred, Lord Tennyson   \n",
       "\n",
       "                                                  Title  Poetry Foundation ID  \\\n",
       "1730                                Break, Break, Break                 45318   \n",
       "2134                    The Charge of the Light Brigade                 45319   \n",
       "2315                                           Claribel                 45320   \n",
       "2687  Crossing the Bar\\n \\n \\n  \\n   Launch Audio in...                 45321   \n",
       "3529                                          The Eagle                 45322   \n",
       "\n",
       "                                                Content  \n",
       "1730  Break, break, break,\\nOn thy cold gray stones,...  \n",
       "2134  I\\n\\nHalf a league, half a league,\\nHalf a lea...  \n",
       "2315  Where Claribel low-lieth\\nThe breezes pause an...  \n",
       "2687  Sunset and evening star,\\nAnd one clear call f...  \n",
       "3529  He clasps the crag with crooked hands;\\nClose ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tennyson_poems = poems_df[poems_df['Author'] == \"Alfred, Lord Tennyson\"]\n",
    "tennyson_poems.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Da4kVNeeKeV"
   },
   "source": [
    "Let's build a dataset of the poems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Akable0ceKeV"
   },
   "outputs": [],
   "source": [
    "all_chars = []\n",
    "dataset = []\n",
    "\n",
    "# cycle through all the poems\n",
    "poem_number = 0\n",
    "for poem in tennyson_poems['Content']:\n",
    "    \n",
    "    # split the poem into its individual characters\n",
    "    poem_characters = list(poem.lower())\n",
    "    dataset.append(poem_characters)\n",
    "    \n",
    "    # Also create a list of all unique characters we saw\n",
    "    for char in poem_characters:\n",
    "        if char not in all_chars:\n",
    "            all_chars.append(char)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQ1w9UQkeKeY"
   },
   "source": [
    "Let's inspect our list of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "pUG0PpzOeKeY",
    "outputId": "b27229c0-512b-4bdc-81f3-a015ea28b844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'r', 'e', 'a', 'k', ',', ' ', '\\n', 'o', 'n', 't', 'h', 'y', 'c', 'l', 'd', 'g', 's', '!', 'i', 'w', 'u', 'm', '.', 'f', \"'\", 'p', 'v', ';', 'x', '“', '”', '?', 'j', '-', 'z', ':', '\"', 'q', '—', '(', ')', 'ë', 'ï', 'æ', '’', 'ä', 'é', 'ö', 'è', '{', '}']\n"
     ]
    }
   ],
   "source": [
    "print(all_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2l4Zi5qYeKea"
   },
   "source": [
    "As well as our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "BxsK0AnneKeb",
    "outputId": "9d23fad1-bbfb-4e52-d7fb-611b825b24a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'r', 'e', 'a', 'k', ',', ' ', 'b', 'r', 'e', 'a', 'k', ',', ' ', 'b', 'r', 'e', 'a', 'k', ',', '\\n', 'o', 'n', ' ', 't', 'h', 'y', ' ', 'c', 'o', 'l', 'd', ' ', 'g', 'r', 'a', 'y', ' ', 's', 't', 'o', 'n', 'e', 's', ',', ' ', 'o', ' ', 's', 'e', 'a', '!', '\\n', 'a', 'n', 'd', ' ', 'i', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 't', 'h', 'a', 't', ' ', 'm', 'y', ' ', 't', 'o', 'n', 'g', 'u', 'e', ' ', 'c', 'o', 'u', 'l', 'd', ' ', 'u', 't', 't', 'e', 'r', '\\n', 't', 'h', 'e', ' ', 't', 'h', 'o', 'u', 'g', 'h', 't', 's', ' ', 't', 'h', 'a', 't', ' ', 'a', 'r', 'i', 's', 'e', ' ', 'i', 'n', ' ', 'm', 'e', '.', '\\n', '\\n', 'o', ',', ' ', 'w', 'e', 'l', 'l', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'f', 'i', 's', 'h', 'e', 'r', 'm', 'a', 'n', \"'\", 's', ' ', 'b', 'o', 'y', ',', '\\n', 't', 'h', 'a', 't', ' ', 'h', 'e', ' ', 's', 'h', 'o', 'u', 't', 's', ' ', 'w', 'i', 't', 'h', ' ', 'h', 'i', 's', ' ', 's', 'i', 's', 't', 'e', 'r', ' ', 'a', 't', ' ', 'p', 'l', 'a', 'y', '!', '\\n', 'o', ',', ' ', 'w', 'e', 'l', 'l', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 's', 'a', 'i', 'l', 'o', 'r', ' ', 'l', 'a', 'd', ',', '\\n', 't', 'h', 'a', 't', ' ', 'h', 'e', ' ', 's', 'i', 'n', 'g', 's', ' ', 'i', 'n', ' ', 'h', 'i', 's', ' ', 'b', 'o', 'a', 't', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'b', 'a', 'y', '!', '\\n', '\\n', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 's', 't', 'a', 't', 'e', 'l', 'y', ' ', 's', 'h', 'i', 'p', 's', ' ', 'g', 'o', ' ', 'o', 'n', '\\n', 't', 'o', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'h', 'a', 'v', 'e', 'n', ' ', 'u', 'n', 'd', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'h', 'i', 'l', 'l', ';', '\\n', 'b', 'u', 't', ' ', 'o', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 't', 'o', 'u', 'c', 'h', ' ', 'o', 'f', ' ', 'a', ' ', 'v', 'a', 'n', 'i', 's', 'h', \"'\", 'd', ' ', 'h', 'a', 'n', 'd', ',', '\\n', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 's', 'o', 'u', 'n', 'd', ' ', 'o', 'f', ' ', 'a', ' ', 'v', 'o', 'i', 'c', 'e', ' ', 't', 'h', 'a', 't', ' ', 'i', 's', ' ', 's', 't', 'i', 'l', 'l', '!', '\\n', '\\n', 'b', 'r', 'e', 'a', 'k', ',', ' ', 'b', 'r', 'e', 'a', 'k', ',', ' ', 'b', 'r', 'e', 'a', 'k', '\\n', 'a', 't', ' ', 't', 'h', 'e', ' ', 'f', 'o', 'o', 't', ' ', 'o', 'f', ' ', 't', 'h', 'y', ' ', 'c', 'r', 'a', 'g', 's', ',', ' ', 'o', ' ', 's', 'e', 'a', '!', '\\n', 'b', 'u', 't', ' ', 't', 'h', 'e', ' ', 't', 'e', 'n', 'd', 'e', 'r', ' ', 'g', 'r', 'a', 'c', 'e', ' ', 'o', 'f', ' ', 'a', ' ', 'd', 'a', 'y', ' ', 't', 'h', 'a', 't', ' ', 'i', 's', ' ', 'd', 'e', 'a', 'd', '\\n', 'w', 'i', 'l', 'l', ' ', 'n', 'e', 'v', 'e', 'r', ' ', 'c', 'o', 'm', 'e', ' ', 'b', 'a', 'c', 'k', ' ', 't', 'o', ' ', 'm', 'e', '.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3x-wT819eKed"
   },
   "source": [
    "Let's discuss what we have so far. We have a data collection where each element is a poem, split up into a list of characters. We want to transform it into a dataset $X$ and $y$, where each data point $x_i$ has a single feature containing a sequence of several characters, and each label $y_i$ is the character that should come after the sequence observed.\n",
    "\n",
    "Suppose we decided on a sequence length of 10, and we explored the first poem:\n",
    "\n",
    "```\n",
    "Break, break, break,\n",
    "On thy cold gray stones, O Sea!\n",
    "And I would that my tongue could utter\n",
    "The thoughts that arise in me.\n",
    "\n",
    "O, well for the fisherman's boy,\n",
    "That he shouts with his sister at play!\n",
    "O, well for the sailor lad,\n",
    "That he sings in his boat on the bay!\n",
    "\n",
    "And the stately ships go on\n",
    "To their haven under the hill;\n",
    "But O for the touch of a vanish'd hand,\n",
    "And the sound of a voice that is still!\n",
    "\n",
    "Break, break, break\n",
    "At the foot of thy crags, O Sea!\n",
    "But the tender grace of a day that is dead\n",
    "Will never come back to me.\n",
    "```\n",
    "\n",
    "We would like our dataset to look like this:\n",
    "\n",
    "$x_0$: `Break, bre`<br>\n",
    "$y_0$: `a`\n",
    "\n",
    "$x_1$: `reak, brea`<br>\n",
    "$y_1$: `k`\n",
    "\n",
    "$x_2$: `eak, break`<br>\n",
    "$y_2$: `,`\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "Let's construct a loop to build this dataset for us. We will use the `collections.deque` object. It is like a list, but once it fills up and we add another element, it kicks the oldest element out. In essence, it is a list that can never be longer than `maxlen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "87E1q2_ueKed"
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 10\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# for each poem\n",
    "for poem in dataset:\n",
    "    char_deque = deque(maxlen=SEQUENCE_LENGTH)\n",
    "    \n",
    "    # go through the characters and place them in a deque, once the deque fills up and we try to add\n",
    "    # another character, the oldest character will be thrown out\n",
    "    for i in range(len(poem)-1):\n",
    "        char = poem[i]\n",
    "        char_deque.append(char)\n",
    "        \n",
    "        if (len(char_deque) == SEQUENCE_LENGTH):\n",
    "            X.append(list(char_deque))\n",
    "            y.append(poem[i+1])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rAu_F2LreKef"
   },
   "source": [
    "Let's inspect our $X$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "TPN9iOzPeKef",
    "outputId": "6cd15194-f932-4454-c2cb-1b18fd377ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: ['b', 'r', 'e', 'a', 'k', ',', ' ', 'b', 'r', 'e']\n",
      "y: a\n",
      "*******\n",
      "X: ['r', 'e', 'a', 'k', ',', ' ', 'b', 'r', 'e', 'a']\n",
      "y: k\n",
      "*******\n",
      "X: ['e', 'a', 'k', ',', ' ', 'b', 'r', 'e', 'a', 'k']\n",
      "y: ,\n",
      "*******\n",
      "X: ['a', 'k', ',', ' ', 'b', 'r', 'e', 'a', 'k', ',']\n",
      "y:  \n",
      "*******\n",
      "X: ['k', ',', ' ', 'b', 'r', 'e', 'a', 'k', ',', ' ']\n",
      "y: b\n",
      "*******\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"X:\",X[i])\n",
    "    print(\"y:\",y[i])\n",
    "    print(\"*******\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNQ3YHcxeKeh"
   },
   "source": [
    "#### Almost there...\n",
    "\n",
    "There are a few more things we have to do, like convert the arrays into numpy arrays for our RNN to consume. However, recall that all machine learning techniques rely on the data being numeric. We will transform the data into numbers in a few stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W34_S7qceKei"
   },
   "source": [
    "First, we will prepare two dictionaries: one to convert characters to numbers, and one to do the conversion back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itTazOureKei"
   },
   "outputs": [],
   "source": [
    "number_to_char = {i: j for i,j in enumerate(all_chars)}\n",
    "char_to_number = {j: i for i,j in enumerate(all_chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1em3tO3DeKek"
   },
   "source": [
    "Now we will convert every character in $X$ and $y$ into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uMWxJkLGeKel"
   },
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    for j in range(len(X[0])):\n",
    "        X[i][j] = char_to_number[X[i][j]]\n",
    "        \n",
    "    y[i] = char_to_number[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "wjWoZ2qqeKem",
    "outputId": "dfb05860-f99f-49c7-feb6-26351cd5e6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0, 1, 2, 3, 4, 5, 6, 0, 1, 2]\n",
      "y: 3\n",
      "*******\n",
      "X: [1, 2, 3, 4, 5, 6, 0, 1, 2, 3]\n",
      "y: 4\n",
      "*******\n",
      "X: [2, 3, 4, 5, 6, 0, 1, 2, 3, 4]\n",
      "y: 5\n",
      "*******\n",
      "X: [3, 4, 5, 6, 0, 1, 2, 3, 4, 5]\n",
      "y: 6\n",
      "*******\n",
      "X: [4, 5, 6, 0, 1, 2, 3, 4, 5, 6]\n",
      "y: 0\n",
      "*******\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"X:\",X[i])\n",
    "    print(\"y:\",y[i])\n",
    "    print(\"*******\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqXzaLJbeKep"
   },
   "source": [
    "Now we will transform our data into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stWs8-o9eKep"
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "G3hvjMSReKes",
    "outputId": "43c06903-378d-4151-dae7-002cd96c6989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185712, 10)\n",
      "(185712,)\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the shapes\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the data shape has to be $n \\times q \\times d$. So we need to reshape this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2IPHGi7seKev"
   },
   "source": [
    "### Thinking Exercise\n",
    "\n",
    "1. How many features do we have?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We have one feature of 10 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7m9h0ZFyeKew",
    "outputId": "272caa86-1b6e-4a19-e67d-9ce24b23da05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185712, 10, 1)\n",
      "(185712,)\n"
     ]
    }
   ],
   "source": [
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ii2sB41eDhNB"
   },
   "source": [
    "We will shuffle the data quite a bit, just to introduce more randomness into how the NN sees the data, so let's create a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zj_3W33eKey"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(X_data, y_data):\n",
    "    \n",
    "    y_data = y_data.reshape((y_data.shape[0], 1, 1))\n",
    "    combined_data = np.hstack((X_data, y_data))\n",
    "    \n",
    "    np.random.shuffle(combined_data)\n",
    "\n",
    "    X_data = combined_data[:, :-1]\n",
    "    y_data = combined_data[:, -1]\n",
    "    \n",
    "    return X_data, y_data.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orQkPef8eKe0"
   },
   "outputs": [],
   "source": [
    "X, y = shuffle_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XaLzbf29nggx",
    "outputId": "031911b3-c5e8-482f-869a-965651f5dfec",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185712, 10, 1)\n",
      "(185712, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle the order of each list not the numbers (chars) in a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqQ0xfBReKe3"
   },
   "source": [
    "### Train and Validation data\n",
    "\n",
    "We're going to split the data into training and validation sets. We don't need a test set here because we don't need a perfect unbiased estimate of how our model will perform on new data. Our unbiased estimate will come from using the trained model to generate some new poetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-ZZuHkoBKZU"
   },
   "outputs": [],
   "source": [
    "# Create training and validation sets\n",
    "validate_set_size = int(0.1 * X.shape[0]) #10% for validation \n",
    "\n",
    "train_set_limit = X.shape[0] - validate_set_size # 90% for training\n",
    "\n",
    "# Split train\n",
    "train_X = X[:train_set_limit]\n",
    "train_y = y[:train_set_limit]\n",
    "\n",
    "# Split validation\n",
    "validation_X = X[train_set_limit : ]\n",
    "validation_y = y[train_set_limit : ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "UCzHepoJBKZZ",
    "outputId": "4013056c-ff81-4702-e4e4-e16e11c22392",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167141, 10, 1)\n",
      "(18571, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)      \n",
    "print(validation_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9O2DqJ8BKZc"
   },
   "source": [
    "First, let's try a regular neural network. To do so, we need to \"flatten\" each of our data points from a 2D tensor to a 1D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XyMtmWvJBKZd",
    "outputId": "32088914-a877-420a-8a97-40dcd752eba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167141, 10)\n",
      "(18571, 10)\n"
     ]
    }
   ],
   "source": [
    "flat_train_X = np.reshape(train_X, (-1, train_X.shape[1]))\n",
    "flat_validation_X = np.reshape(validation_X, (-1, validation_X.shape[1]))\n",
    "\n",
    "print(flat_train_X.shape)\n",
    "print(flat_validation_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us visualize RNNs later, let's look at a visualization of our Feed-Forward Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1ur2OAo1U5tMhzdlEhjiQckj2ZnazRdRq\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    " \n",
    "<center> <i>(Image prepared using http://alexlenail.me/NN-SVG/)</i> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $n$ is the size of our input.\n",
    "\n",
    "The first layer will read our flattened input (the characters turned into numbers) and the network will perform a forward pass through the layers, until the output layer outputs a probability distribution over our 52 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be constructing rather large networks, we will be working with pre-trained networks and simply load them to make them work, but we will still review the code to build the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's declare a `Sequential` model. This means all layers will be added in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN_model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add our first layer. Since this is our first layer we need to explicitly specify the shape of the input (the shape of each data point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN_model.add(Dense(1024, activation='relu', input_shape=(flat_train_X.shape[1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we are using the `relu` activation function. Recall that function is a function which returns either 0 or $x$, whichever is larger.\n",
    "\n",
    "$$ReLU(x) = max(0, x)$$\n",
    "\n",
    "Let's also add some dropout and batch normalization. Dropout will drop out some random output from this layer at every training pass, and batch normalization will make sure output from this layer has mean 0, and standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN_model.add(Dropout(0.2))          # randomly drop 20% of the previous layer output\n",
    "FNN_model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another layer of `relu` neurons, along with more dropout and batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN_model.add(Dense(612, activation='relu'))\n",
    "FNN_model.add(Dropout(0.2))\n",
    "FNN_model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add another layer of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN_model.add(Dense(32, activation='relu'))\n",
    "FNN_model.add(Dropout(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add an output layer that will give us a probability distribution. It will have as many neurons as we have classes (52 in this case), and will use the `softmax` function to make sure all outputs are fractions between 0 and 1, and they all add up to 1.\n",
    "\n",
    "Recall that the softmax function is defined on a vector of outputs $\\vec{O} = [o_1, o_2, o_3, ..., o_n]$\n",
    "\n",
    "$$softmax(o_i) = \\frac{e^{o_i}}{\\Sigma_{i=1}^n e^{o_i}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_number = len(all_chars) #52\n",
    "\n",
    "FNN_model.add(Dense(class_number, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BraqvrILBKZk"
   },
   "outputs": [],
   "source": [
    "\n",
    "sgd = SGD(lr=0.01, decay=0.0, momentum=0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "# Compile model. Still need to compile even if we load from H5 file since the model must be compiled\n",
    "# to do predictions\n",
    "FNN_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SFiaCFOcBKZl",
    "outputId": "dcfad1ee-e7b1-4184-bda4-8a4a828f5513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 1024)              11264     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 612)               627300    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 612)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 612)               2448      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                19616     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 52)                1716      \n",
      "=================================================================\n",
      "Total params: 666,440\n",
      "Trainable params: 663,168\n",
      "Non-trainable params: 3,272\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Return the model summary\n",
    "FNN_model.summary()\n",
    "\n",
    "# Save an image of the model's architecture to a file\n",
    "plot_model(FNN_model, to_file='Feed Forward NN.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could train the model now, but since this could take a while (more so for the future model), we'll use a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8efXAzUoBKZo",
    "outputId": "f0778fc1-0dc4-4797-b99e-50b34a5c1abb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights model from disk\n",
      "No need to train, model is fully trained\n",
      "18571/18571 [==============================] - 1s 34us/step\n",
      "Accuracy score: 25.1%\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('FNN_model.h5'):\n",
    "    EPOCHS = 40       # NNs operate in epochs, meaning this is how many times the neural network will go through the entire data\n",
    "    BATCH_SIZE = 480   # at each epoch, it will split the data into units of 480 samples, and train on those\n",
    "\n",
    "    FNN_model.fit(\n",
    "        flat_train_X, train_y,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(flat_validation_X, validation_y))\n",
    "    \n",
    "    # great way to save the model as a json/h5 file set \n",
    "    # https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "    model_json = FNN_model.to_json()\n",
    "    with open(\"FNN_model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    FNN_model.save_weights(\"FNN_model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "else:\n",
    "    FNN_model.load_weights(\"FNN_model.h5\")\n",
    "    print(\"Loaded weights model from disk\") \n",
    "    print(\"No need to train, model is fully trained\")\n",
    "    loss_score, accuracy_score = FNN_model.evaluate(flat_validation_X, validation_y, batch_size=480, verbose=1)\n",
    "    print(\"Accuracy score: \"+str(round(accuracy_score*100,2))+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8psL0kHoExM"
   },
   "source": [
    "Now let's check how well this performs. We'll build a loop that will continiously produce some character for us based on a sequence we will \"seed\" the network with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ACu_AKzEoOlY",
    "outputId": "33a48cec-07e0-484b-956e-81414f3aaf10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be.rr mnredsäsel fhg,vnatend baoteaamtoodenntherso rea\n",
      "anh nundse,vore'toma\n",
      "herrldsdy,ohetdge ie deasdt kni syæee foe be d\"\n",
      "ote mtfnha dh ga\n",
      "ssaeyi led wle yhe ald lopl aari th wtu sl tot\n",
      " figl rridg th \n"
     ]
    }
   ],
   "source": [
    "input_phrase = \"To be or not to be.\"\n",
    "\n",
    "# we will predict 200 characters forward after the input_phrase\n",
    "for i in range(200):\n",
    "    \n",
    "    # get the last 10 characters of our input_phrase and convert them to numbers\n",
    "    network_input = list(input_phrase[-SEQUENCE_LENGTH:])\n",
    "    for j in range(len(network_input)):\n",
    "        network_input[j] = char_to_number[network_input[j]]\n",
    "    # convert into an array then reshape it to explicitly have 1 feature\n",
    "    network_input = np.array(network_input)\n",
    "    network_input = network_input.reshape((1, SEQUENCE_LENGTH))\n",
    "\n",
    "    # get probabilistic predictions from the neural network\n",
    "    # randomly draw a single predicted character from the full list with their probabilities determined by the network's prediction\n",
    "    predict_proba = FNN_model.predict(network_input)[0]\n",
    "    predict_char = np.random.choice(all_chars, 1, p = predict_proba)[0]\n",
    "\n",
    "    input_phrase += predict_char\n",
    "\n",
    "print(input_phrase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sa5Pf75BBKZr"
   },
   "source": [
    "### Hmmmm....\n",
    "\n",
    "Not great results. This doesn't look like English even.\n",
    "\n",
    "So now let's see how an RNN can help us. Let's inspect our series data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "gAJIeohypR6u",
    "outputId": "86a69376-f0bb-49e4-f147-0327b33b2116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167141, 10, 1)\n",
      "(18571, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(validation_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And build our RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Fhn7JS-BKZt"
   },
   "outputs": [],
   "source": [
    "RNN_model1 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bRq7k2UQBKZv"
   },
   "source": [
    "Now, let's add our first layer. We always need to specify an input size for our first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGHNHBszBKZw"
   },
   "outputs": [],
   "source": [
    "RNN_model1.add(LSTM(1024, activation='relu', input_shape=(train_X.shape[1:]), return_sequences=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBoR75fzBKZy"
   },
   "source": [
    "Notice something interesting, we are asking our recurrent layer to return a sequence (`return_sequences=True`). Why? \n",
    "\n",
    "Well, if we want to add a second recurrent layer, it must accept a sequence as an input from the first layer. In this case, if each data point has 1 feature (our sequence) that contains a 10-measurement series, then the output of this layer will be of size $10 \\times 1024$ as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0tpqwrmjBKZy"
   },
   "source": [
    "Let's add some dropout and batch normalization for good measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "faqD2BGkBKZ0"
   },
   "outputs": [],
   "source": [
    "RNN_model1.add(Dropout(0.2))\n",
    "RNN_model1.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MIWQtJDxBKZ1"
   },
   "source": [
    "Now let's add a second recurrent layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zG5ylvcWBKZ2"
   },
   "outputs": [],
   "source": [
    "RNN_model1.add(LSTM(612, activation='relu'))\n",
    "RNN_model1.add(Dropout(0.2))\n",
    "RNN_model1.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ceqfq3xpBKZ4"
   },
   "source": [
    "Notice this time we are not returning a series, the output shape will be only $1 \\times 612$ (rather than $10 \\times 612$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1VhFu8OBKZ5"
   },
   "outputs": [],
   "source": [
    "RNN_model1.add(Dense(32, activation='relu'))\n",
    "RNN_model1.add(Dropout(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MpmEtG7oBKZ7"
   },
   "source": [
    "Finally, let's add an output layer. Since we are dealing with classification, our default activation is the softmax function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "74f6g8uvBKZ7"
   },
   "outputs": [],
   "source": [
    "RNN_model1.add(Dense(class_number, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBvzpOnABKZ9"
   },
   "source": [
    "And there we go! We have a model! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1aRiD7_DYgn7h068DMtHrKmuV4GcnyQvV\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "<center> <i>Image prepared using http://alexlenail.me/NN-SVG/)</i> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $n$ is again the number of input data points, but notice now the first two layers process 2D tensors each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, since the model is too big and takes a long time to train, we will load a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rv_npeAPBKZ9"
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01, decay=0.0, momentum=0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "# Compile model\n",
    "RNN_model1.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EFtVheveBKZ-",
    "outputId": "c0fa989e-b6eb-4292-8e28-34bf91962cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 10, 1024)          4202496   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 10, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 612)               4007376   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 612)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 612)               2448      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                19616     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 52)                1716      \n",
      "=================================================================\n",
      "Total params: 8,237,748\n",
      "Trainable params: 8,234,476\n",
      "Non-trainable params: 3,272\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display its summary\n",
    "RNN_model1.summary()\n",
    "\n",
    "# Save an image of its architecture to file\n",
    "plot_model(RNN_model1, to_file='RNN_model1.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "id": "_CmvSWSABKaA",
    "outputId": "e7b3d995-aeca-429a-9ee7-2a7f97941a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 167141 samples, validate on 18571 samples\n",
      "Epoch 1/3\n",
      "  1440/167141 [..............................] - ETA: 27:38 - loss: 4.1264 - acc: 0.0257"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-d46808fe9929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                    validation_data=(validation_X, validation_y))\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN_model1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists('RNN_model1.h5'):\n",
    "    EPOCHS = 3       # NNs operate in epochs, meaning this is how many times the neural network will go through \n",
    "                      # the entire data\n",
    "    BATCH_SIZE = 480   # at each epoch, it will split the data into units of 48 samples, and train on those\n",
    "\n",
    "\n",
    "    RNN_model1.fit(train_X, train_y,\n",
    "                   batch_size=BATCH_SIZE,\n",
    "                   epochs=EPOCHS,\n",
    "                   validation_data=(validation_X, validation_y))\n",
    "    \n",
    "    model_json = RNN_model1.to_json()\n",
    "    with open(\"RNN_model1.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    RNN_model1.save_weights(\"RNN_model1.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "else:\n",
    "    # load weights into new model\n",
    "    print(\"Loading weights from h5 file\")\n",
    "    RNN_model1.load_weights(\"RNN_model1.h5\")\n",
    "    print(\"Loaded weights from disk\")\n",
    "    print(\"No need to train, model is fully trained\")\n",
    "    loss_score, accuracy_score = RNN_model1.evaluate(validation_X, validation_y, batch_size=480, verbose=1)\n",
    "    print(\"Accuracy score: \"+str(round(accuracy_score*100,2))+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xGMPh2lBKaC"
   },
   "source": [
    "Our model is trained, let's get some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DzKM0HKOBKaC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be.as that ituma oitdl the a wmgik bug the ratkt igsg fryisoerfdd\n",
      "britnë(\n",
      "berankirerr;ofel we gor, and wnri bocer n'ei—ruutl la winti the rd,s ofecomettowt, imlp\n",
      "fe bearteik shum pitie tnand alh monr,\n",
      "sa\n"
     ]
    }
   ],
   "source": [
    "input_phrase = \"To be or not to be.\"\n",
    "\n",
    "# we will predict 200 characters forward after the input_phrase\n",
    "for i in range(200):\n",
    "    \n",
    "    # get the last 10 characters of our input_phrase and convert them to numbers\n",
    "    network_input = list(input_phrase[-SEQUENCE_LENGTH:])\n",
    "    for j in range(len(network_input)):\n",
    "        network_input[j] = char_to_number[network_input[j]]\n",
    "    # convert into an array then reshape it to explicitly have 1 feature\n",
    "    network_input = np.array(network_input)\n",
    "    network_input = network_input.reshape((1, SEQUENCE_LENGTH, 1))\n",
    "\n",
    "    # get probabilistic predictions from the neural network\n",
    "    # randomly draw a single predicted character from the full list with their probabilities determined by the network's prediction\n",
    "    predict_proba = RNN_model1.predict(network_input)[0]\n",
    "    predict_char = np.random.choice(all_chars, 1, p = predict_proba)[0]\n",
    "\n",
    "    input_phrase += predict_char\n",
    "    print(i, end=\"\\r\")\n",
    "    \n",
    "print(input_phrase)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6zTCT6_wtjx"
   },
   "source": [
    "This isn't much better. To get RNNs working with text better, we need to embed our vocabulary. Recall that word embedding means taking each word and embedding into a high dimensional space as a vector. Turns out we can do the same with characters, or really any finite collection.\n",
    "\n",
    "Keras offers us an embedding layer as part of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Exercise 1\n",
    "\n",
    "1. Look at the Keras documentation for embedding layers, and find out how to add an embedding layer to our model before the recurrent LSTM layer. Use an embedding dimension of 30 (i.e. embed the characters into a 30 dimensional space). dimension is arbitraty but smaller than input_dim.\n",
    "\n",
    "*Warning: The code below this exercise will not run if the embedding layer (part of this exercise) isn't added!* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case-specific embedding of words for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
    "\n",
    "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n",
    "\n",
    "It is a flexible layer that can be used in a variety of ways, such as:\n",
    "\n",
    "It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
    "It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
    "It can be used to load a pre-trained word embedding model, a type of transfer learning.\n",
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "It must specify 3 arguments:\n",
    "\n",
    "input_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "\n",
    "\n",
    "output_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "\n",
    "\n",
    "input_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model2 = Sequential()\n",
    "# add something here as part of exercise\n",
    "RNN_model2.add(Embedding(52, 30, input_length=10))\n",
    "\n",
    "RNN_model2.add(LSTM(1024, activation='relu', return_sequences=True)) #input_shape=(train_X.shape[1:]), is not necessary\n",
    "RNN_model2.add(Dropout(0.2))\n",
    "RNN_model2.add(BatchNormalization())\n",
    "\n",
    "RNN_model2.add(LSTM(612, activation='relu'))\n",
    "RNN_model2.add(Dropout(0.2))\n",
    "RNN_model2.add(BatchNormalization())\n",
    "\n",
    "RNN_model2.add(Dense(32, activation='relu'))\n",
    "RNN_model2.add(Dropout(0.1))\n",
    "\n",
    "RNN_model2.add(Dense(class_number, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN with the embedding layer will look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1hgdDOMNow465V4Bs6cOFYAY4Fon2cLO3\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APoyZFJvxIeg"
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01, decay=0.0, momentum=0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "# Compile model\n",
    "RNN_model2.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hGO8pz65xIg3",
    "outputId": "85431795-7c7c-402b-d086-f1a6aaed2295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 10, 30)            1560      \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 10, 1024)          4321280   \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 10, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 612)               4007376   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 612)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 612)               2448      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                19616     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 52)                1716      \n",
      "=================================================================\n",
      "Total params: 8,358,092\n",
      "Trainable params: 8,354,820\n",
      "Non-trainable params: 3,272\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display its summary\n",
    "RNN_model2.summary()\n",
    "\n",
    "# Save an image of its architecture to file\n",
    "plot_model(RNN_model2, to_file='RNN_model2.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "HwAaVG7mxIj-",
    "outputId": "5c7c57a3-694b-4d3f-e57b-0917bfa0b0f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from h5 file\n",
      "Loaded weights from disk\n",
      "No need to train, model is fully trained\n",
      "18571/18571 [==============================] - 60s 3ms/step\n",
      "Accuracy score: 38.15%\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('RNN_model2.json'):\n",
    "    EPOCHS = 40       # NNs operate in epochs, meaning this is how many times the neural network will go through \n",
    "                      # the entire data\n",
    "    BATCH_SIZE = 480   # at each epoch, it will split the data into units of 480 samples, and train on those\n",
    "\n",
    "    train_X = train_X.reshape((-1, SEQUENCE_LENGTH))\n",
    "    model.fit(\n",
    "        train_X, train_y,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(validation_X.reshape((-1, SEQUENCE_LENGTH)), validation_y))\n",
    "    \n",
    "    model_json = RNN_model2.to_json()\n",
    "    with open(\"RNN_model2.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    RNN_model2.save_weights(\"RNN_model2.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "else:\n",
    "    print(\"Loading weights from h5 file\")\n",
    "    RNN_model2.load_weights(\"RNN_model2.h5\")\n",
    "    print(\"Loaded weights from disk\")\n",
    "    print(\"No need to train, model is fully trained\")\n",
    "    loss_score, accuracy_score = RNN_model2.evaluate(validation_X.reshape((-1, SEQUENCE_LENGTH)), \n",
    "                                                     validation_y, \n",
    "                                                     batch_size=480, \n",
    "                                                     verbose=1)\n",
    "    print(\"Accuracy score: \"+str(round(accuracy_score*100,2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "1WGvoohe4D6G",
    "outputId": "15951cdc-059c-4bb0-dfea-555fbd26fcfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be.e\n",
      "queile ar sakearns, unldd thy halk boddy legatl wheid cha moat.\n",
      "\n",
      "a“ve }the roer that gron of lows crike ponortdedre in sa me? hyhe the, clapkreor he rihgt rad—odl onsleg and orf memenn’d \"hetr.\n",
      "\n",
      "ite\n"
     ]
    }
   ],
   "source": [
    "input_phrase = \"To be or not to be.\"\n",
    "\n",
    "# we will predict 200 characters forward after the input_phrase\n",
    "for i in range(200):\n",
    "    \n",
    "    # get the last 10 characters of our input_phrase and convert them to numbers\n",
    "    network_input = list(input_phrase[-SEQUENCE_LENGTH:])\n",
    "    for j in range(len(network_input)):\n",
    "        network_input[j] = char_to_number[network_input[j]]\n",
    "    # convert into an array then reshape it to explicitly have 1 feature\n",
    "    network_input = np.array(network_input)\n",
    "    network_input = network_input.reshape((1, SEQUENCE_LENGTH))\n",
    "\n",
    "    # get probabilistic predictions from the neural network\n",
    "    # randomly draw a single predicted character from the full list with their probabilities determined by the network's prediction\n",
    "    predict_proba = RNN_model2.predict(network_input)[0]\n",
    "    predict_char = np.random.choice(all_chars, 1, p = predict_proba)[0]\n",
    "\n",
    "\n",
    "    input_phrase += predict_char\n",
    "    print(i, end=\"\\r\")\n",
    "\n",
    "print(input_phrase)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6iXa3G85wce"
   },
   "source": [
    "This seems a bit more English like, and we actually will get better results if we train the model for longer. However, we can approach this problem a different way, we can split our data into words instead of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h6W1FbTP59FQ"
   },
   "source": [
    "### Splitting the Data Into Words\n",
    "\n",
    "So far, we've split the data character by character. This process has a two-fold benefit. First, the embedding is much smaller (only $10 \\times 52$) so it is faster to learn, but also the predictions are only divided into 52 classes.\n",
    "\n",
    "However, the system spends the majority of its training time learning correct word spelling. While we could potentially train the system longer and employ some auto-correction to fix spelling errors, we can instead split the poems dataset into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wq2SU6LE4u6q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['break', 'break', 'break', '\\n', 'on', 'thy', 'cold', 'gray', 'stones', 'o', 'sea', '\\n', 'and', 'i', 'would', 'that', 'my', 'tongue', 'could', 'utter', '\\n', 'the', 'thoughts', 'that', 'arise', 'in', 'me', '\\n', '\\n', 'o', 'well', 'for', 'the', 'fishermans', 'boy', '\\n', 'that', 'he', 'shouts', 'with', 'his', 'sister', 'at', 'play', '\\n', 'o', 'well', 'for', 'the', 'sailor', 'lad', '\\n', 'that', 'he', 'sings', 'in', 'his', 'boat', 'on', 'the', 'bay', '\\n', '\\n', 'and', 'the', 'stately', 'ships', 'go', 'on', '\\n', 'to', 'their', 'haven', 'under', 'the', 'hill', '\\n', 'but', 'o', 'for', 'the', 'touch', 'of', 'a', 'vanishd', 'hand', '\\n', 'and', 'the', 'sound', 'of', 'a', 'voice', 'that', 'is', 'still', '\\n', '\\n', 'break', 'break', 'break', '\\n', 'at', 'the', 'foot', 'of', 'thy', 'crags', 'o', 'sea', '\\n', 'but', 'the', 'tender', 'grace', 'of', 'a', 'day', 'that', 'is', 'dead', '\\n', 'will', 'never', 'come', 'back', 'to', 'me']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "all_words = []\n",
    "dataset = []\n",
    "\n",
    "# cycle through all the poems\n",
    "for poem in tennyson_poems['Content']:\n",
    "    poem = poem.lower()\n",
    "    \n",
    "    # Regex method for removing special characters from a string.\n",
    "    # [^A-Za-z0-9 \\n]+ matches all characters that are NOT alphanumeric, a space, or '\\n'\n",
    "    # https://stackoverflow.com/questions/5843518/remove-all-special-characters-punctuation-and-spaces-from-string\n",
    "    poem_transformed = re.sub('[^A-Za-z0-9 \\n]+', '', poem)\n",
    "    \n",
    "        \n",
    "    # Notice now we are splitting by spaces rather than turning the poem into a list of characters\n",
    "    poem_transformed = poem_transformed.replace('\\n', ' \\n ').replace('  ', ' ')\n",
    "    # split the poem into its individual words\n",
    "    poem_words = poem_transformed.split(' ')\n",
    "    dataset.append(poem_words)\n",
    "    \n",
    "    # Also create a list of all unique words we saw\n",
    "    for word in poem_words:\n",
    "        if word not in all_words:\n",
    "            all_words.append(word)\n",
    "\n",
    "    \n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many classes we are working with when splitting by words rather than by characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class number when splitting into words: 5318\n"
     ]
    }
   ],
   "source": [
    "print(\"class number when splitting into words:\",len(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the poem dataset is split, we will convert it into the same format of $X$ and $y$ as we've had before. Notice we are also keeping the newline character (`\\n`) as a separate word so the system will learn when to place a newline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91Vjz18j4u8-"
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 10\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# for each poem\n",
    "for poem in dataset:\n",
    "    word_deque = deque(maxlen=SEQUENCE_LENGTH)\n",
    "    \n",
    "    # go through the words and place them in a deque, once the deque fills up and we try to add\n",
    "    # another word, the oldest word will be thrown out\n",
    "    for i in range(len(poem)-1):\n",
    "        word = poem[i]\n",
    "        word_deque.append(word)\n",
    "        \n",
    "        if (len(word_deque) == SEQUENCE_LENGTH):\n",
    "            X.append(list(word_deque))\n",
    "            y.append(poem[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect our $X$ and $y$ as always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tHB8sb4U4u_5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: ['break', 'break', 'break', '\\n', 'on', 'thy', 'cold', 'gray', 'stones', 'o']\n",
      "y: sea\n",
      "*******\n",
      "X: ['break', 'break', '\\n', 'on', 'thy', 'cold', 'gray', 'stones', 'o', 'sea']\n",
      "y: \n",
      "\n",
      "*******\n",
      "X: ['break', '\\n', 'on', 'thy', 'cold', 'gray', 'stones', 'o', 'sea', '\\n']\n",
      "y: and\n",
      "*******\n",
      "X: ['\\n', 'on', 'thy', 'cold', 'gray', 'stones', 'o', 'sea', '\\n', 'and']\n",
      "y: i\n",
      "*******\n",
      "X: ['on', 'thy', 'cold', 'gray', 'stones', 'o', 'sea', '\\n', 'and', 'i']\n",
      "y: would\n",
      "*******\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"X:\",X[i])\n",
    "    print(\"y:\",y[i])\n",
    "    print(\"*******\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build our word to numbers and numbers to words converters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_to_word = {i: j for i,j in enumerate(all_words)}\n",
    "word_to_number = {j: i for i,j in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    for j in range(len(X[0])):\n",
    "        X[i][j] = word_to_number[X[i][j]]\n",
    "        \n",
    "    y[i] = word_to_number[y[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate our shapes, and split into a train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38865, 10)\n",
      "(38865,)\n",
      "(38865, 10, 1)\n",
      "(38865,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "# Let's look at the shapes\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38865, 10, 1)\n",
      "(38865, 1)\n"
     ]
    }
   ],
   "source": [
    "X, y = shuffle_data(X, y)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34979, 10, 1)\n",
      "(3886, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create train, validate, and test data\n",
    "validate_set_size = int(0.1 * X.shape[0])\n",
    "\n",
    "train_set_limit = X.shape[0] - validate_set_size\n",
    "\n",
    "# Split train\n",
    "train_X = X[:train_set_limit]\n",
    "train_y = y[:train_set_limit]\n",
    "\n",
    "# Split validation\n",
    "validation_X = X[train_set_limit : ]\n",
    "validation_y = y[train_set_limit : ]\n",
    "\n",
    "print(train_X.shape)      \n",
    "print(validation_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as before, the model could take very long to train so we will use a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_number = len(all_words)\n",
    "\n",
    "    \n",
    "RNN_model3 = Sequential()\n",
    "# This time we will embed the words into a higher dimensional space, 300-dimensional\n",
    "RNN_model3.add(Embedding(len(all_words), 300, input_length=SEQUENCE_LENGTH))\n",
    "\n",
    "RNN_model3.add(LSTM(1024, activation='relu', return_sequences=True))\n",
    "RNN_model3.add(Dropout(0.2))\n",
    "RNN_model3.add(BatchNormalization())\n",
    "\n",
    "RNN_model3.add(LSTM(612, activation='relu'))\n",
    "RNN_model3.add(Dropout(0.2))\n",
    "RNN_model3.add(BatchNormalization())\n",
    "\n",
    "RNN_model3.add(Dense(32, activation='relu'))\n",
    "RNN_model3.add(Dropout(0.1))\n",
    "\n",
    "RNN_model3.add(Dense(class_number, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new network will look as follows. Notice the only change is in the output layer size, and the embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=15UWSQFYNKEyKNVN7jTyuRlddHWITENlz\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01, decay=0.0, momentum=0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "# Compile model\n",
    "RNN_model3.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 10, 300)           1595400   \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 10, 1024)          5427200   \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 10, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 612)               4007376   \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 612)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 612)               2448      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                19616     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 5318)              175494    \n",
      "=================================================================\n",
      "Total params: 11,231,630\n",
      "Trainable params: 11,228,358\n",
      "Non-trainable params: 3,272\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display its summary\n",
    "RNN_model3.summary()\n",
    "\n",
    "# Save an image of its architecture to file\n",
    "plot_model(RNN_model3, to_file='RNN_model3.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from h5 file\n",
      "Loaded weights from disk\n",
      "No need to train, model is fully trained\n",
      "3886/3886 [==============================] - 13s 3ms/step\n",
      "Accuracy score: 23.06%\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('RNN_model3.json'):\n",
    "    EPOCHS = 40       # NNs operate in epochs, meaning this is how many times the neural network will go through \n",
    "                      # the entire data\n",
    "    BATCH_SIZE = 480   # at each epoch, it will split the data into units of 48 samples, and train on those\n",
    "\n",
    "    train_X = train_X.reshape((-1, SEQUENCE_LENGTH))\n",
    "    RNN_model3.fit(train_X, train_y,\n",
    "                   batch_size=BATCH_SIZE,\n",
    "                   epochs=EPOCHS,\n",
    "                   validation_data=(validation_X.reshape((-1, SEQUENCE_LENGTH)), validation_y))\n",
    "else:\n",
    "    print(\"Loading weights from h5 file\")\n",
    "    RNN_model3.load_weights('RNN_model3.h5')\n",
    "    print(\"Loaded weights from disk\")\n",
    "    print(\"No need to train, model is fully trained\")\n",
    "    loss_score, accuracy_score = RNN_model3.evaluate(validation_X.reshape((-1, SEQUENCE_LENGTH)), \n",
    "                                                     validation_y, \n",
    "                                                     batch_size=480, \n",
    "                                                     verbose=1)\n",
    "    print(\"Accuracy score: \"+str(round(accuracy_score*100,2))+\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we now have 5,318 possible classes (for words), so ~22% accuracy is a pretty good score for this problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model, we want to check its performance in completing a sentence.\n",
    "Recall before we used the phrase `to be or not to be` as the seed and let the model complete the sentence from there character by character.\n",
    "\n",
    "If we use the same phrase, we need to pre-process it a bit first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "1. Use the phrase below, and pre-process it so it can be used by the `RNN_model3` network we've built (remember the input must be individual words, and that the network expects an input of length 10 every time we request a word as output)\n",
    "2. For all four network training exercises, we've carved out a validation set. What would we use it for during the training process (hint: it may be too costly to train the network for 100 epochs only to find out its peak performance is at 50 epochs)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from h5 file\n",
      "Loaded weights from disk\n",
      "No need to train, model is fully trained\n",
      "3886/3886 [==============================] - 12s 3ms/step\n",
      "Accuracy score: 23.06%\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('RNN_model3.json'):\n",
    "    EPOCHS = 40       # NNs operate in epochs, meaning this is how many times the neural network will go through \n",
    "                      # the entire data\n",
    "    BATCH_SIZE = 480   # at each epoch, it will split the data into units of 480 samples, and train on those\n",
    "\n",
    "    train_X = train_X.reshape((-1, SEQUENCE_LENGTH))\n",
    "    model.fit(\n",
    "        train_X, train_y,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(validation_X.reshape((-1, SEQUENCE_LENGTH)), validation_y))\n",
    "    \n",
    "    model_json = RNN_modele.to_json()\n",
    "    with open(\"RNN_model3.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    RNN_model2.save_weights(\"RNN_model3.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "else:\n",
    "    print(\"Loading weights from h5 file\")\n",
    "    RNN_model3.load_weights(\"RNN_model3.h5\")\n",
    "    print(\"Loaded weights from disk\")\n",
    "    print(\"No need to train, model is fully trained\")\n",
    "    loss_score, accuracy_score = RNN_model3.evaluate(validation_X.reshape((-1, SEQUENCE_LENGTH)), \n",
    "                                                     validation_y, \n",
    "                                                     batch_size=480, \n",
    "                                                     verbose=1)\n",
    "    print(\"Accuracy score: \"+str(round(accuracy_score*100,2))+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"dear sir I heard you like the sea\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' and 'p' must have same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-233d8b298ff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# randomly draw a single predicted character from the full list with their probabilities determined by the network's prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredict_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN_model3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpredict_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' and 'p' must have same size"
     ]
    }
   ],
   "source": [
    "input_phrase = \"To be or not to be.\"\n",
    "\n",
    "# we will predict 20 characters forward after the input_phrase\n",
    "for i in range(20):\n",
    "    \n",
    "    # get the last 10 characters of our input_phrase and convert them to numbers\n",
    "    network_input = list(input_phrase[-SEQUENCE_LENGTH:])\n",
    "    for j in range(len(network_input)):\n",
    "        network_input[j] = char_to_number[network_input[j]]\n",
    "    # convert into an array then reshape it to explicitly have 1 feature\n",
    "    network_input = np.array(network_input)\n",
    "    network_input = network_input.reshape((1, SEQUENCE_LENGTH))\n",
    "\n",
    "    # get probabilistic predictions from the neural network\n",
    "    # randomly draw a single predicted character from the full list with their probabilities determined by the network's prediction\n",
    "    predict_proba = RNN_model3.predict(network_input)[0]\n",
    "    predict_char = np.random.choice(all_chars, 1, p = predict_proba)[0]\n",
    "\n",
    "\n",
    "    input_phrase += predict_char\n",
    "    print(i, end=\"\\r\")\n",
    "\n",
    "print(input_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJp3V8cwBKaY"
   },
   "source": [
    "<div id=\"container\" style=\"position:relative;\">\n",
    "<div style=\"position:relative; float:right\"><img style=\"height:25px\"\"width: 50px\" src =\"https://drive.google.com/uc?export=view&id=14VoXUJftgptWtdNhtNYVm6cjVmEWpki1\" />\n",
    "</div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "byQreJfoBKaU"
   ],
   "name": "Introduction to Recurrent Neural Networks v2.0.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
